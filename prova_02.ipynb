{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e35c0f9",
   "metadata": {},
   "source": [
    "#### Para execução do código extraia os dados para a pasta Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383851b",
   "metadata": {},
   "source": [
    "# Importações e organização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variáveis\n",
    "meta_data_path = 'Data/sinais.csv'\n",
    "data_folder = 'Data/Sinais/'\n",
    "\n",
    "random_state = 12\n",
    "\n",
    "#tratamento dos keypoints\n",
    "reference_kp_ids = [23, 24]   # todas as coordenadas dos keypoints vão ser recalculadas relativamente à posição média destes keypoints\n",
    "visibility_threshold = 0.1    # se a visibilidade de um keypoint for menor que este valor, o keypoint é ignorado\n",
    "\n",
    "n_segments_temporal_embedding = 15\n",
    "\n",
    "\n",
    "pca_explainability = 0.95\n",
    "\n",
    "kmeans_range = range(2, 40)\n",
    "use_elbow_method = True    # se True, o k é escolhido pelo método do cotovelo; se False, é usado o valor definido em manually_chosen_k\n",
    "manually_chosen_k =25\n",
    "\n",
    "kmeans_iterations = 20 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bb108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando os Meta-dados\n",
    "meta_data = pd.read_csv(meta_data_path)\n",
    "\n",
    "#verificando valores nulos\n",
    "meta_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36818d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removendo valores nulos\n",
    "meta_data = meta_data.dropna()\n",
    "\n",
    "\"\"\"\n",
    "    - Como vou estabelecer um keypoint de referência, as dimensões do vídeo deixam de importar.\n",
    "    - Com o objetivo de generalizar os modelos, o interprete também não deve ser levado em consideração.\n",
    "    - Já que o frame rate é fixo em 30 fps para todos os vídeos, número de frames e duração são redundantes.\n",
    "\"\"\"\n",
    "meta_data = meta_data.drop(columns=['width', 'height', 'interprete', 'duration_sec'])\n",
    "\n",
    "#carregando dados\n",
    "data = []\n",
    "for index, row in meta_data.iterrows():\n",
    "    file_path = data_folder + row['file_name']\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    signal_data = {\n",
    "        'label': row['sinal'],\n",
    "        'num_frames': row['num_frames'],\n",
    "        'frames': json_data['frames']\n",
    "    }\n",
    "    data.append(signal_data)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace37c39",
   "metadata": {},
   "source": [
    "# Pré-processamento dos keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se todos os frames possuem 33 keypoints                \n",
    "cout = 0\n",
    "for signal in data:\n",
    "    for frame in signal['frames']:\n",
    "        if len(frame['keypoints']) != 33:\n",
    "            cout += 1\n",
    "            print(len(frame['keypoints']))\n",
    "print(f'quantidade de frames com menos de 33 kp: {cout}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo frames que não possuem 33 keypoints\n",
    "for signal in data:\n",
    "    frames_filtrados = [frame for frame in signal['frames'] if len(frame['keypoints']) == 33]\n",
    "    signal['frames'] = frames_filtrados\n",
    "    signal['num_frames'] = len(frames_filtrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Como dito antes vamos definir um ponto de referência e então calcular o X, Y e Z de todos os keypoints a partir desse ponto de referência,\n",
    "    dessa forma alem de não dependermos da altura e largura da imagem, a posição de cada keypoint passa a ser relativa ao corpo do interprete,\n",
    "    tornando a representação mais consistente entre vídeos.\n",
    "\"\"\"\n",
    "def pre_process_keypoints(data):\n",
    "    for signal in data:\n",
    "        for frame in signal['frames']:\n",
    "            \n",
    "            # Calculo do ponto de referência médio\n",
    "            ref_points = [frame['keypoints'][i] for i in reference_kp_ids]\n",
    "            ref_x = np.mean([p['x'] for p in ref_points])\n",
    "            ref_y = np.mean([p['y'] for p in ref_points])\n",
    "            ref_z = np.mean([p['z'] for p in ref_points])\n",
    "            \n",
    "            \n",
    "            # Ajuste dos keypoints em relação ao ponto de referência\n",
    "            for kp in frame['keypoints']:\n",
    "                kp['x'] -= ref_x\n",
    "                kp['y'] -= ref_y\n",
    "                kp['z'] -= ref_z\n",
    "                \n",
    "pre_process_keypoints(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Calcula média, mínimo e máximo de visibilidade para cada keypoint\n",
    "\"\"\"\n",
    "\n",
    "def compute_visibility_stats(data, n_keypoints=33):\n",
    "   \n",
    "    visibility_sums = np.zeros(n_keypoints)\n",
    "    counts = np.zeros(n_keypoints)\n",
    "    visibility_min = np.full(n_keypoints, np.inf)\n",
    "    visibility_max = np.full(n_keypoints, -np.inf)\n",
    "    \n",
    "    for signal in data:\n",
    "        for frame in signal['frames']:\n",
    "            for kp in frame['keypoints']:\n",
    "                id = kp['id']\n",
    "                visibility = kp['visibility']\n",
    "                visibility_sums[id] += visibility\n",
    "                counts[id] += 1\n",
    "                visibility_min[id] = min(visibility_min[id], visibility)\n",
    "                visibility_max[id] = max(visibility_max[id], visibility)\n",
    "\n",
    "    mean_visibility = visibility_sums / counts\n",
    "    return mean_visibility, visibility_min, visibility_max\n",
    "    \n",
    "\n",
    "mean_vis, min_vis, max_vis = compute_visibility_stats(data)\n",
    "\n",
    "\n",
    "#Analise da visibilidade dos keypoints\n",
    "kp_to_keep = [i for i, visibility in enumerate(mean_vis) if visibility >= visibility_threshold]\n",
    "\n",
    "\n",
    "#exibindo resultados\n",
    "for i in range(33):\n",
    "    print(f'Keypoint {i}: Mean={mean_vis[i]:.3f}, Min={min_vis[i]:.3f}, Max={max_vis[i]:.3f}')\n",
    "    \n",
    "print(f'Keypoints a serem mantidos (visibilidade média >= {visibility_threshold}): {kp_to_keep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffa015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção dos keypoints com visibilidade baixa\n",
    "for signal in data:\n",
    "    for frame in signal['frames']:\n",
    "        frame['keypoints'] = [kp for kp in frame['keypoints'] if kp['id'] in kp_to_keep]\n",
    "        \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b2fd1",
   "metadata": {},
   "source": [
    "# Preparação dos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Para não perder muita informação temporal, vamos dividir os frames em segmentos e calcular \n",
    "    a média dos keypoints em cada segmento.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def temporal_embedding(data, n_segments):\n",
    "    for signal in data:\n",
    "        n_frames = signal['num_frames']\n",
    "        segment_size = n_frames // n_segments\n",
    "        embedded_frames = []\n",
    "        # Divide os frames em segmentos temporais\n",
    "        for i in range(n_segments):\n",
    "            start = i * segment_size\n",
    "            # O último segmento pega até o final\n",
    "            end = (i + 1) * segment_size if i < n_segments - 1 else n_frames\n",
    "            \n",
    "            segment = signal['frames'][start:end]\n",
    "            \n",
    "            # soma dos dados de todos os frames do segmento\n",
    "            frames_sum = {\n",
    "                'frame': i,\n",
    "                'keypoints': [{'id': kp['id'], 'x': 0, 'y': 0, 'z': 0, 'visibility': 0} for kp in segment[0]['keypoints']]\n",
    "            }\n",
    "            for frame in segment:\n",
    "                for i, kp in enumerate(frame['keypoints']):\n",
    "                    frames_sum['keypoints'][i]['x'] += kp['x']\n",
    "                    frames_sum['keypoints'][i]['y'] += kp['y']\n",
    "                    frames_sum['keypoints'][i]['z'] += kp['z']\n",
    "                    frames_sum['keypoints'][i]['visibility'] += kp['visibility']\n",
    "                    \n",
    "            # Calcula a média dos keypoints para o segmento\n",
    "            avg_frame = {\n",
    "                'frame': frames_sum['frame'],\n",
    "                'keypoints': [{'id': kp['id'],\n",
    "                               'x': kp['x'] / len(segment),\n",
    "                               'y': kp['y'] / len(segment),\n",
    "                               'z': kp['z'] / len(segment),\n",
    "                               'visibility': kp['visibility'] / len(segment)} for kp in frames_sum['keypoints']]\n",
    "            }\n",
    "            # Adiciona o frame médio à lista de embeddings\n",
    "            embedded_frames.append(avg_frame)\n",
    "        signal['frames'] = embedded_frames\n",
    "        signal['num_frames'] = len(embedded_frames)\n",
    "    \n",
    "\n",
    "temporal_embedding(data, n_segments_temporal_embedding)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1e6f7",
   "metadata": {},
   "source": [
    "# Converção para data frame e PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converção data para DataFrame\n",
    "for signal in data:\n",
    "    features = {}\n",
    "\n",
    "    for frame in signal['frames']:\n",
    "        for kp in frame['keypoints']:\n",
    "            for cord in ['x', 'y', 'z', 'visibility']:\n",
    "                features[f'f_{frame[\"frame\"]}_kp_{kp[\"id\"]}_{cord}'] = kp[cord]\n",
    "    signal['features'] = features\n",
    "    \n",
    "df = pd.DataFrame([{'label': signal['label'], **signal['features']} for signal in data])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISE DO DATAFRAME\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"INFORMAÇÕES BÁSICAS DO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Shape do DataFrame: {df.shape}\")\n",
    "print(f\"Número de amostras: {df.shape[0]}\")\n",
    "print(f\"Número de features: {df.shape[1] - 1}\")\n",
    "print(f\"Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DISTRIBUIÇÃO DAS CLASSES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nNúmero de classes únicas: {df['label'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82412e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Tendo em vista a quantidade de features, vamos aplicar PCA para reduzir a dimensionalidade.\n",
    "\"\"\"\n",
    "# Separação de X e y\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "test_pca = PCA(random_state=random_state)\n",
    "X_pca = test_pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calcula a variância explicada acumulada pelos componentes principais\n",
    "explained_variance = test_pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "pca_df = pd.DataFrame({\n",
    "    'Componente': range(1, len(explained_variance) + 1),\n",
    "    'Variância_Explicada': explained_variance\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=pca_df, x='Componente', y='Variância_Explicada', \n",
    "             marker='o', linewidth=2.5, markersize=6, color='blue', markeredgecolor='blue')\n",
    "plt.xlabel(\"Número de Componentes\", fontsize=12)\n",
    "plt.ylabel(\"Variância Explicada Acumulada\", fontsize=12)\n",
    "plt.title(\"PCA - Variância Explicada Acumulada\", fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Pode-se observar que conseguimos uma explicabilidade proxima de 100% com menos de 200 componentes. \n",
    "\"\"\"\n",
    "\n",
    "for exp_var in np.arange(0.95, 0.991, 0.01):\n",
    "    pca_tmp = PCA(n_components=exp_var, random_state=random_state)\n",
    "    pca_tmp.fit(X_scaled)\n",
    "    print(f'Explicabilidade: {exp_var:.2f} -> Número de componentes: {pca_tmp.n_components_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02015614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com 95% consegue-se uma grande redução de dimensionalidade sem muita perda de informação\n",
    "\n",
    "pca = PCA(n_components=pca_explainability, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61c0a6",
   "metadata": {},
   "source": [
    "# Análise Supervisionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação dos labels para que f1_score consiga interpretar\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "#scoring\n",
    "scoring = make_scorer(f1_score, average='macro', zero_division=0)\n",
    "\n",
    "#definição kfold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "#definição dos pipelines\n",
    "preprocessing = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', pca)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"Random_forest\": Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('clf', RandomForestClassifier(random_state=random_state))\n",
    "    ]),\n",
    "       \n",
    "\n",
    "    \"KNN\": Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ]),\n",
    "\n",
    "    \"MPL\": Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('clf', MLPClassifier(max_iter=500, random_state=random_state))\n",
    "    ])}\n",
    "\n",
    "#score dos modelos\n",
    "\n",
    "scores = pd.DataFrame({\n",
    "    'Random_forest': cross_val_score(models['Random_forest'], X, y_encoded, cv=cv, n_jobs=-1, scoring=scoring),\n",
    "    'KNN': cross_val_score(models['KNN'], X, y_encoded, cv=cv, n_jobs=-1, scoring=scoring),\n",
    "    'MPL': cross_val_score(models['MPL'], X, y_encoded, cv=cv, n_jobs=-1, scoring=scoring)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d6153",
   "metadata": {},
   "source": [
    "# Avaliação dos modelos supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatando o DataFrame\n",
    "scores_melted = scores.melt(var_name='Modelo', value_name='F1-Score')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Modelo', y='F1-Score', hue='Modelo', data=scores_melted, palette='viridis', legend=False)\n",
    "plt.title('Comparação do F1-Score entre Modelos', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Modelo', fontsize=12)\n",
    "plt.ylabel('F1-Score', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Médias e desvio padrão\n",
    "print(\"=\" * 50)\n",
    "print(\"MÉDIA E DESVIO PADRÃO DO F1-SCORE\")\n",
    "print(\"=\" * 50)\n",
    "print(scores.agg(['mean', 'std']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86744422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predições de cada modelo\n",
    "y_pred_rf = cross_val_predict(models['Random_forest'], X, y_encoded, cv=cv, n_jobs=-1,)\n",
    "y_pred_knn = cross_val_predict(models['KNN'], X, y_encoded, cv=cv, n_jobs=-1)\n",
    "y_pred_mpl = cross_val_predict(models['MPL'], X, y_encoded, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Matrizes de confusão\n",
    "cm_rf = confusion_matrix(y_encoded, y_pred_rf)\n",
    "cm_knn = confusion_matrix(y_encoded, y_pred_knn)\n",
    "cm_mpl = confusion_matrix(y_encoded, y_pred_mpl)\n",
    "\n",
    "# Nomes das classes\n",
    "class_names = label_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "    Plotando as matrizes de confusão\n",
    "'''\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 24))\n",
    "fig.suptitle('Matrizes de Confusão', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Random Forest\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[0].set_title('Random Forest', fontsize=16)\n",
    "axes[0].set_xlabel('Predito', fontsize=14)\n",
    "axes[0].set_ylabel('Verdadeiro', fontsize=14)\n",
    "\n",
    "# KNN\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1].set_title('KNN', fontsize=16)\n",
    "axes[1].set_xlabel('Predito', fontsize=14)\n",
    "axes[1].set_ylabel('Verdadeiro', fontsize=14)\n",
    "\n",
    "# MLP\n",
    "sns.heatmap(cm_mpl, annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[2].set_title('MLP', fontsize=16)\n",
    "axes[2].set_xlabel('Predito', fontsize=14)\n",
    "axes[2].set_ylabel('Verdadeiro', fontsize=14)\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1b364",
   "metadata": {},
   "source": [
    "# Análise não supervisionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be40b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pré-processamento nos dados\n",
    "X_processed = preprocessing.fit_transform(X)\n",
    "\n",
    "inertia = []\n",
    "\n",
    "#K-means para diferentes valores de K\n",
    "for k in kmeans_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "    kmeans.fit(X_processed)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.lineplot(x=list(kmeans_range), y=inertia, marker='o', linewidth=2.5, markersize=8, color='purple')\n",
    "plt.title('Teste do Cotovelo', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Número de Clusters (K)', fontsize=14)\n",
    "plt.ylabel('Inércia', fontsize=14)\n",
    "plt.xticks(kmeans_range)\n",
    "plt.grid(True, alpha=0.4, linestyle='--')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f62249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(inertia, k_range, threshold=0.05):\n",
    "   # diferença relativa entre as inércias consecutivas\n",
    "    diffs = np.diff(inertia) / inertia[:-1]\n",
    "\n",
    "    for i, diff in enumerate(diffs):\n",
    "        if abs(diff) < threshold:\n",
    "            return list(k_range)[i+1]\n",
    "    # Se não encontrar, retorna o maior k\n",
    "    return list(k_range)[-1]\n",
    "\n",
    "#definição do valor de k\n",
    "if use_elbow_method:\n",
    "    chosen_k = find_best_k(inertia, kmeans_range, threshold=0.05)\n",
    "else:\n",
    "    chosen_k = manually_chosen_k\n",
    "print(f'Número ideal de clusters (K) encontrado: {chosen_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67668d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means com o K escolhido\n",
    "kmeans_final = KMeans(n_clusters=chosen_k, random_state=random_state, n_init=kmeans_iterations)\n",
    "clusters_kmeans = kmeans_final.fit_predict(X_processed)\n",
    "\n",
    "# Clustering hierárquico (Ward)\n",
    "ward = AgglomerativeClustering(n_clusters=chosen_k, linkage='ward')\n",
    "clusters_ward = ward.fit_predict(X_processed)\n",
    "\n",
    "# Clustering hierárquico (Average)\n",
    "average = AgglomerativeClustering(n_clusters=chosen_k, linkage='average')\n",
    "clusters_average = average.fit_predict(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca622cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"AVALIAÇÃO DOS ALGORITMOS - SILHOUETTE SCORE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Silhouette Score de cada método\n",
    "silhouette_kmeans = silhouette_score(X_processed, clusters_kmeans)\n",
    "silhouette_ward = silhouette_score(X_processed, clusters_ward)\n",
    "silhouette_average = silhouette_score(X_processed, clusters_average)\n",
    "\n",
    "print(f\"Silhouette Score - K-means: {silhouette_kmeans:.4f}\")\n",
    "print(f\"Silhouette Score - Hierárquico (Ward): {silhouette_ward:.4f}\")\n",
    "print(f\"Silhouette Score - Hierárquico (Average): {silhouette_average:.4f}\")\n",
    "\n",
    "# Determinando o melhor método\n",
    "scores = {\n",
    "    'K-means': silhouette_kmeans,\n",
    "    'Hierárquico (Ward)': silhouette_ward,\n",
    "    'Hierárquico (Average)': silhouette_average\n",
    "}\n",
    "\n",
    "melhor_metodo = max(scores, key=scores.get)\n",
    "print(f\"\\nMelhor método baseado no Silhouette Score: {melhor_metodo} ({scores[melhor_metodo]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Análise da distribuição dos labels reais em cada cluster\n",
    "\n",
    "# K-means\n",
    "plt.figure(figsize=(16, 8))\n",
    "ct_kmeans = pd.crosstab(y, clusters_kmeans)\n",
    "sns.heatmap(ct_kmeans, cmap='Blues', annot=False, fmt='d')\n",
    "plt.title('Distribuição dos Labels Reais nos Clusters - K-means', fontsize=16)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Label Real')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hierárquico (Ward)\n",
    "plt.figure(figsize=(16, 8))\n",
    "ct_ward = pd.crosstab(y, clusters_ward)\n",
    "sns.heatmap(ct_ward, cmap='Greens', annot=False, fmt='d')\n",
    "plt.title('Distribuição dos Labels Reais nos Clusters - Hierárquico (Ward)', fontsize=16)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Label Real')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hierárquico (Average)\n",
    "plt.figure(figsize=(16, 8))\n",
    "ct_avg = pd.crosstab(y, clusters_average)\n",
    "sns.heatmap(ct_avg, cmap='Oranges', annot=False, fmt='d')\n",
    "plt.title('Distribuição dos Labels Reais nos Clusters - Hierárquico (Average)', fontsize=16)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Label Real')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
